{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Генерация текста с помощью RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом ноутбуке сначала необходимо:\n",
    "1. Разобрать пример с гугла по ссылке (https://www.tensorflow.org/tutorials/text/text_generation?hl=ru)\n",
    "1. Постараться преобразовать пример в рабочую модель для генерации daily analysis статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "Length of text: 1115394 characters\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'\\xd1\\x84', b'\\xd1\\x8b', b'\\xd0\\xb2', b'\\xd0\\xb0'],\n",
       "       [b'\\xd0\\xbe', b'\\xd0\\xbb', b'\\xd0\\xb4', b'\\xd0\\xb6']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['фыва', 'олдж']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[1, 1, 1, 1], [1, 1, 1, 1]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Преобразование символов в векторную строку\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'[UNK]', b'[UNK]', b'[UNK]', b'[UNK]'], [b'[UNK]', b'[UNK]', b'[UNK]', b'[UNK]']]>\n"
     ]
    }
   ],
   "source": [
    "# Обратное преобразование\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача прогнозирования\n",
    "\n",
    "Для данного символа или последовательности символов какой следующий символ наиболее вероятен? Это задача, которой вы обучаете модель. Входными данными для модели будет последовательность символов, и вы обучаете модель предсказывать выходные данные - следующий символ на каждом временном шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создавайте обучающие примеры и цели\n",
    "\n",
    "Затем разделите текст на примеры последовательностей. Каждая входная последовательность будет содержать символы seq_length из текста.\n",
    "\n",
    "Для каждой входной последовательности соответствующие целевые объекты содержат текст одинаковой длины, за исключением смещения на один символ вправо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2], dtype=int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n",
      "now Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us ki\n",
      "ll him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be d\n",
      "one: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citi\n"
     ]
    }
   ],
   "source": [
    "# Объединяем обратно\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вот функция, которая принимает последовательность как ввод, дублирует и сдвигает ее, \n",
    "# чтобы выровнять ввод и метку для каждого временного шага\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "Target: irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy().decode('utf-8'))\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создавайте тренировочные партии\n",
    "\n",
    "Вы использовали tf.data чтобы разбить текст на управляемые последовательности. Но прежде чем вводить эти данные в модель, вам необходимо перетасовать данные и упаковать их в пакеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постройте модель\n",
    "\n",
    "В этом разделе модель определяется как подкласс keras.Model (подробнее см. Создание новых слоев и моделей через создание подклассов ).\n",
    "\n",
    "Эта модель состоит из трех слоев:\n",
    "\n",
    "* tf.keras.layers.Embedding : входной слой. Обучаемая таблица поиска, которая будет отображать каждый идентификатор символа в вектор с размерами embedding_dim ;\n",
    "* tf.keras.layers.GRU : тип RNN с размером units=rnn_units (здесь также можно использовать слой LSTM.)\n",
    "* tf.keras.layers.Dense : выходной слой с выходными значениями vocab_size . Для каждого символа в словаре используется один логит. Это логарифмическая вероятность каждого символа в соответствии с моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(ids_from_chars.get_vocabulary()))\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуйте модель\n",
    "\n",
    "Теперь запустите модель, чтобы убедиться, что она ведет себя должным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17152     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  68675     \n",
      "=================================================================\n",
      "Total params: 4,024,131\n",
      "Trainable params: 4,024,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 26, 11, 50,  4, 20,  4, 31, 46, 44, 28, 21, 48, 57,  0, 26, 47,\n",
       "       11, 40, 32,  6, 61, 48, 66, 41, 30, 30, 56, 36, 57, 29, 27, 54,  5,\n",
       "       23, 32, 64, 64, 20, 11, 66, 40, 30, 40, 63, 36, 13, 59, 22, 28, 65,\n",
       "       49, 31, 27, 16,  3, 24, 22, 53, 60, 55, 22, 10, 53, 60, 49,  7,  7,\n",
       "       10, 31, 51, 27, 28, 41, 47, 60, 63, 36,  2,  9, 13, 60, 54, 63, 23,\n",
       "       23, 64, 20, 52, 60,  9, 62,  9, 39, 19, 45,  2, 12,  3, 60],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' sufficient to serve it?\\n\\nELBOW:\\nFaith, sir, few of any wit in such matters: as they\\nare chosen, the'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"TL3j!F!QfdNGhqLg3ZR&uhzaPPpVqOMn$IRxxF3zZPZwV;sHNyiQMB JHmtoH.mti''.QkMNagtwV\\n-;tnwIIxFlt-v-YEe\\n: t\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучите модель\n",
    "\n",
    "На этом этапе проблему можно рассматривать как стандартную задачу классификации. Учитывая предыдущее состояние RNN и ввод этого временного шага, спрогнозируйте класс следующего символа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.205507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.05457"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Прикрепите оптимизатор и функцию потерь\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройте процедуру обучения с tf.keras.Model.compile метода tf.keras.Model.compile . \n",
    "# Используйте tf.keras.optimizers.Adam с аргументами по умолчанию и функцией потерь.\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используйте tf.keras.callbacks.ModelCheckpoint чтобы убедиться, что контрольные точки сохраняются во время обучения:\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  4/172 [..............................] - ETA: 4:04 - loss: 4.1101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-42e62d0a1b3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Провести обучение\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Провести обучение\n",
    "EPOCHS = 5\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать текст\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OXFORD:\n",
      "Yet withdefore flood, and call'd on. Not give them, my poor\n",
      "Mansal humbland aw, let much made\n",
      "your sintly and him, my dost to son,\n",
      "And be bone in door age.\n",
      "\n",
      "BRUSTOND:\n",
      "Why Nore.\n",
      "\n",
      "ANGELO:\n",
      "Diedesly for it. Assister or clomem, they\n",
      "Ware, for husband; for your hang-to hear me so speak.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Marry she to denight the morning high of\n",
      "When ever his montunes bount your old with suchis name to do wet you\n",
      "honey on husband, and with such flumber, and love to Fray your wrench\n",
      "That give fible cur \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.8199756145477295\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['OXFORD:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(500):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"apple: he would de\\nyours. Bry Velint:\\nI you strike him here: they bryalking them in\\nAbove enemy, hear me not. Say this yep unto my streed?\\nNot full, mine encive, I should have die to those\\nAnd 'Be that, no\"], shape=(1,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.33849358558654785\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['apple'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация аналитики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ТРЕНД ВВЕРХ;Изменение цены за последние полгода составляет 26.39%, а за последний месяц 11.72%. Напр'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_analfile = 'gen_text.csv'\n",
    "an_text = open(path_to_analfile, 'rb').read().decode(encoding='utf-8')\n",
    "an_vocab = sorted(set(an_text))\n",
    "print('{} unique characters'.format(len(an_vocab)))\n",
    "an_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([b'\\xd0\\xa2', b'\\xd0\\xa0', b'\\xd0\\x95', b'\\xd0\\x9d', b'\\xd0\\x94',\n",
      "       b' ', b'\\xd0\\x92', b'\\xd0\\x92', b'\\xd0\\x95', b'\\xd0\\xa0'],\n",
      "      dtype=object)\n",
      " array([b';', b'\\xd0\\x98', b'\\xd0\\xb7', b'\\xd0\\xbc', b'\\xd0\\xb5',\n",
      "       b'\\xd0\\xbd', b'\\xd0\\xb5', b'\\xd0\\xbd', b'\\xd0\\xb8'], dtype=object)]\n",
      "<tf.RaggedTensor [[45, 43, 36, 40, 35, 4, 33, 33, 36, 43], [19, 38, 59, 64, 57, 65, 57, 65, 60]]>\n",
      "[array([b'\\xd0\\xa2', b'\\xd0\\xa0', b'\\xd0\\x95', b'\\xd0\\x9d', b'\\xd0\\x94',\n",
      "       b' ', b'\\xd0\\x92', b'\\xd0\\x92', b'\\xd0\\x95', b'\\xd0\\xa0'],\n",
      "      dtype=object)\n",
      " array([b';', b'\\xd0\\x98', b'\\xd0\\xb7', b'\\xd0\\xbc', b'\\xd0\\xb5',\n",
      "       b'\\xd0\\xbd', b'\\xd0\\xb5', b'\\xd0\\xbd', b'\\xd0\\xb8'], dtype=object)]\n",
      "tf.Tensor(\n",
      "[b'\\xd0\\xa2\\xd0\\xa0\\xd0\\x95\\xd0\\x9d\\xd0\\x94 \\xd0\\x92\\xd0\\x92\\xd0\\x95\\xd0\\xa0'\n",
      " b';\\xd0\\x98\\xd0\\xb7\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8'], shape=(2,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilya\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:1950: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(rows)\n"
     ]
    }
   ],
   "source": [
    "# Примеры и проверка работы с текстом и преобразованиями\n",
    "example_texts = [an_text[:10], an_text[11:20]]\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "print(chars.numpy())\n",
    "\n",
    "# Преобразование символов в векторную строку\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(an_vocab))\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)\n",
    "\n",
    "# Обратное преобразование\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
    "chars = chars_from_ids(ids)\n",
    "print(chars.numpy())\n",
    "print(tf.strings.reduce_join(chars, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([45 43 36 ... 65 78  8], shape=(2932,), dtype=int64)\n",
      "Т\n",
      "Р\n",
      "Е\n",
      "Н\n",
      "Д\n",
      "tf.Tensor(\n",
      "[b'\\xd0\\xa2' b'\\xd0\\xa0' b'\\xd0\\x95' b'\\xd0\\x9d' b'\\xd0\\x94' b' '\n",
      " b'\\xd0\\x92' b'\\xd0\\x92' b'\\xd0\\x95' b'\\xd0\\xa0' b'\\xd0\\xa5' b';'\n",
      " b'\\xd0\\x98' b'\\xd0\\xb7' b'\\xd0\\xbc' b'\\xd0\\xb5' b'\\xd0\\xbd' b'\\xd0\\xb5'\n",
      " b'\\xd0\\xbd' b'\\xd0\\xb8' b'\\xd0\\xb5' b' ' b'\\xd1\\x86' b'\\xd0\\xb5'\n",
      " b'\\xd0\\xbd' b'\\xd1\\x8b' b' ' b'\\xd0\\xb7' b'\\xd0\\xb0' b' ' b'\\xd0\\xbf'\n",
      " b'\\xd0\\xbe' b'\\xd1\\x81' b'\\xd0\\xbb' b'\\xd0\\xb5' b'\\xd0\\xb4' b'\\xd0\\xbd'\n",
      " b'\\xd0\\xb8' b'\\xd0\\xb5' b' ' b'\\xd0\\xbf' b'\\xd0\\xbe' b'\\xd0\\xbb'\n",
      " b'\\xd0\\xb3' b'\\xd0\\xbe' b'\\xd0\\xb4' b'\\xd0\\xb0' b' ' b'\\xd1\\x81'\n",
      " b'\\xd0\\xbe' b'\\xd1\\x81'], shape=(51,), dtype=string)\n",
      "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'], ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])\n",
      "Input : ТРЕНД ВВЕРХ;Изменение цены за последние полгода со\n",
      "Target: РЕНД ВВЕРХ;Изменение цены за последние полгода сос\n",
      "Input : тавляет 26.39%, а за последний месяц 11.72%. Напра\n",
      "Target: авляет 26.39%, а за последний месяц 11.72%. Направ\n",
      "<PrefetchDataset shapes: ((32, 50), (32, 50)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# Создаем набор данных полный\n",
    "\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(an_text, 'UTF-8'))\n",
    "print(all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for ids in ids_dataset.take(5):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
    "    \n",
    "# Разбиваем на батчи по 50 символов\n",
    "seq_length = 50\n",
    "examples_per_epoch = len(an_text)//(seq_length+1)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n",
    "\n",
    "# Вот функция, которая принимает последовательность как ввод, дублирует и сдвигает ее, \n",
    "# чтобы выровнять ввод и метку для каждого временного шага\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "print(split_input_target(list(\"Tensorflow\")))\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy().decode('utf-8'))\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy().decode('utf-8'))\n",
    "\n",
    "# Финализируем датасет\n",
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "an_vocab_size = len(an_vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "class MyAnModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.lstm = tf.keras.layers.LSTM(4, return_sequences=True, \n",
    "                                   return_state=True),\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 83) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "model = MyAnModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(2):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_an_model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      multiple                  21248     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  multiple                  3938304   \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You tried to call `count_params` on lstm_2, but the layer isn't built. You can build it manually via: `lstm_2.build(batch_input_shape)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   2357\u001b[0m                               \u001b[0mline_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m                               \u001b[0mpositions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2359\u001b[1;33m                               print_fn=print_fn)\n\u001b[0m\u001b[0;32m   2360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\layer_utils.py\u001b[0m in \u001b[0;36mprint_summary\u001b[1;34m(model, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m    251\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequential_like\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m       \u001b[0mprint_layer_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m       \u001b[0mprint_layer_summary_with_connections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\layer_utils.py\u001b[0m in \u001b[0;36mprint_layer_summary\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mcls_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' ('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m')'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mprint_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pandora\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mcount_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2157\u001b[0m                          \u001b[1;34m', but the layer isn\\'t built. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m                          \u001b[1;34m'You can build it manually via: `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m                          '.build(batch_input_shape)`.')\n\u001b[0m\u001b[0;32m   2160\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You tried to call `count_params` on lstm_2, but the layer isn't built. You can build it manually via: `lstm_2.build(batch_input_shape)`."
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26 39 52 18 77 41 14 59 10 79 77 44 51 44 41 38  8 11 75 64 67 63  5 50\n",
      " 37 10 64 28 77 33 60 26 56  3 80 26  5 55 78  0 65 57 68 11 81 59 12  5\n",
      " 69 38]\n",
      "Input:\n",
      " b'\\xd0\\x9d\\xd0\\x98\\xd0\\x97;\\xd0\\x98\\xd0\\xb7\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd1\\x86\\xd0\\xb5\\xd0\\xbd\\xd1\\x8b \\xd0\\xb7\\xd0\\xb0 \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd0\\xb5\\xd0\\xb4\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd0\\xb3\\xd0\\xbe\\xd0\\xb4\\xd0\\xb0 \\xd1\\x81\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f\\xd0\\xb5\\xd1\\x82'\n",
      "\n",
      "Next Char Predictions:\n",
      " aЛа:щО6з1ьщСЯСОИ.2чмпл%ЭЗ1мlщВиaд\r",
      "эa%гынер2юз3%сИ\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(sampled_indices)\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 50, 83)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.419919\n",
      "83.089554\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 4.4198\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 4.3853\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 4.3344\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 4.1986\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 3.9147\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 4.0440\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 4.1301\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 4.1337\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 4.1150\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 4.0963\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 4.0476\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 3.9668\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 3.8596\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 3.7219\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 3.5066\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 3.6919\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 3.5555\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 3.4285\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 3.4042\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 3.3447\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 3.3888\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 3.3376\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 3.3275\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 3.3273\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 3.2549\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 3.2341\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 3.2218\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 3.1748\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 3.1557\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 3.1298\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 3.1559\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 3.0749\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 3.0338\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 3.0518\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 3.0489\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 2.9640\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 2.9766\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 2.9412\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 2.9180\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 2.8214\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 2.8661\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 2.7793\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 2.7794\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 2.7796\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 2.7232\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 2.7424\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 2.6100\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 2.6333\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 2.6269\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 2.5660\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 2.5193\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 2.5425\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 2.3902\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 2.4891\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 2.4097\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 2.4098\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 2.3678\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 2.3560\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 2.3643\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 2.2850\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 2.2877\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 2.2622\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 2.2268\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 2.1679\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 2.0840\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 2.1674\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 2.0239\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 2.0311\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 2.0253\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 2.0517\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 1.9588\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 2.0120\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 1.9496\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 1.9281\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 1.9248\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 1.8410\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 1.8224\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 1.8494\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 1.7628\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 1.7757\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.7675\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.7018\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 1.7556\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 774ms/step - loss: 1.7155\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 1.6912\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 1.6070\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 1.5880\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 1.5684\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.4888\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 1.5617\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.4308\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 1.3850\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 1.3985\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 1.3142\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 1.2269\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 1.2313\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 1.1902\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 1.3048\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 1.1481\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.2161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 445ms/step - loss: 1.2161\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "# Прикрепите оптимизатор и функцию потерь\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "print(tf.exp(mean_loss).numpy())\n",
    "\n",
    "# Настройте процедуру обучения с tf.keras.Model.compile метода tf.keras.Model.compile . \n",
    "# Используйте tf.keras.optimizers.Adam с аргументами по умолчанию и функцией потерь.\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Используйте tf.keras.callbacks.ModelCheckpoint чтобы убедиться, что контрольные точки сохраняются во время обучения:\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# Провести обучение\n",
    "EPOCHS = 100\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать текст\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СИГНАЛ;Лина поворить о икатором э дваноследне пикутстсяветь горио, сродначное, иринах.\r\n",
      "ТНТВ;Ц\r",
      "aД9 В1Р1ИИИИ \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.8305926322937012\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['СИГНАЛ;'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
